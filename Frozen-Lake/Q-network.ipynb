{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning for Frozen-Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "#Python version : 3.11.9\n",
    "import gym            #version : 0.26.2\n",
    "import numpy as np    #version : 2.0.2\n",
    "import random\n",
    "import tensorflow as tf     #version : 2.18.0\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)  # Use a deterministic FrozenLake environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feed-forward network used to choose actions\n",
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # Register W as a trainable variable using add_weight\n",
    "        self.W = self.add_weight(\n",
    "            name=\"W\", \n",
    "            shape=(16, 4), \n",
    "            initializer=tf.random_uniform_initializer(minval=0, maxval=0.01), \n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.W)\n",
    "\n",
    "# Initialize the network\n",
    "q_network = QNetwork()\n",
    "print(q_network.trainable_variables)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "nextQ = tf.constant([[0.0] * 4], dtype=tf.float32)  # Replace placeholder with a constant/tensor\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()  # Replace reduce_sum with a standard loss function\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the netwrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning parameters\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 100\n",
    "\n",
    "# Create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    g_state = env.reset()  # Reset the environment\n",
    "    # print(state)\n",
    "    state = g_state[0]\n",
    "    rAll = 0  # Total reward for the episode\n",
    "    truncated = False\n",
    "    step = 0\n",
    "\n",
    "    while step < 99:\n",
    "        step += 1\n",
    "\n",
    "        # Choose an action: epsilon-greedy policy\n",
    "        state_one_hot = np.identity(16)[state:state+1]  # One-hot encoding of the state\n",
    "        Q_values = q_network(tf.constant(state_one_hot, dtype=tf.float32)).numpy()\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_values)\n",
    "\n",
    "        # Take the action, get the reward and the next state\n",
    "        next_state, reward, term, truncated, info_ = env.step(action)\n",
    "        print(next_state, reward, term, truncated, info_)\n",
    "\n",
    "        # Obtain the Q' values for the next state\n",
    "        next_state_one_hot = np.identity(16)[next_state:next_state + 1]\n",
    "        Q_values_next = q_network(tf.constant(next_state_one_hot, dtype=tf.float32)).numpy()\n",
    "        max_Q_next = np.max(Q_values_next)\n",
    "\n",
    "        # Update Q-values using the Bellman equation\n",
    "        target_Q = Q_values.copy()\n",
    "        target_Q[0, action] = reward + gamma * max_Q_next\n",
    "\n",
    "        # Train the network\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_out = q_network(tf.constant(state_one_hot, dtype=tf.float32))\n",
    "            loss = tf.reduce_sum(tf.square(Q_out - tf.constant(target_Q, dtype=tf.float32)))\n",
    "\n",
    "        gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "        # filtered_gradients_and_vars = [(g, v) for g, v in zip(gradients, q_network.trainable_variables) if g is not None]\n",
    "        optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "        rAll += reward\n",
    "        state = next_state\n",
    "\n",
    "        if truncated:\n",
    "            # Reduce epsilon (exploration rate) over time\n",
    "            epsilon = 1.0 / ((episode / 50) + 10)\n",
    "            break\n",
    "\n",
    "    jList.append(step)\n",
    "    rList.append(rAll)\n",
    "\n",
    "print(sum(rList))\n",
    "print(f\"Percent of successful episodes: {sum(rList) / num_episodes * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(jList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
